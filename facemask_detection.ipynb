{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "015128ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: imutils in c:\\users\\dell\\downloads\\new folder (3)\\lib\\site-packages (0.5.4)\n"
     ]
    }
   ],
   "source": [
    " !pip install imutils\n",
    "\n",
    "\n",
    "# A series of convenience functions to make basic image processing functions such as translation,\n",
    "# rotation, resizing, skeletonization, displaying Matplotlib images, sorting contours, detecting edges,\n",
    "# and much more easier with OpenCV and both Python 2.7 and Python 3.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a67d1f24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in c:\\users\\dell\\downloads\\new folder (3)\\lib\\site-packages (4.7.0.72)\n",
      "Requirement already satisfied: numpy>=1.21.2 in c:\\users\\dell\\downloads\\new folder (3)\\lib\\site-packages (from opencv-python) (1.23.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python\n",
    "\n",
    "# OpenCV-Python is a library of Python bindings designed to solve computer vision problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f8bcc1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.layers import AveragePooling2D\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from imutils import paths\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# ImageDataGenerator is a class in Keras (a high-level deep learning library) that allows for real-time data augmentation during training of deep neural networks. It provides various methods to generate augmented images from the original dataset.  \n",
    "\n",
    "# MobileNetV2 is a pre-trained deep learning model architecture available in Keras. It is a variant of the MobileNet architecture designed for mobile and embedded vision applications. The import statement makes the MobileNetV2 model available for use in the code.\n",
    "# python        \n",
    "\n",
    "# These import statements import various layer classes from Keras that are commonly used in constructing deep neural networks.\n",
    "# AveragePooling2D performs average pooling operation on 2D spatial data.\n",
    "# Dropout applies dropout regularization to the input, randomly setting a fraction of input units to 0 during training.\n",
    "# Flatten flattens the input, transforming a multi-dimensional tensor into a 1D vector.\n",
    "# Dense is a fully connected layer, where each input node is connected to each output node.\n",
    "# Input is used to instantiate a Keras tensor.    \n",
    "\n",
    "# Model is a class in Keras that represents a deep learning model. It is used to create custom models by specifying the input and output layers.  \n",
    "\n",
    "# Adam is an optimization algorithm commonly used for training deep neural networks. It adapts the learning rate during training to improve convergence.        \n",
    "\n",
    "# preprocess_input is a function specific to the MobileNetV2 model provided by Keras. It preprocesses the input image data by scaling and normalizing it according to the requirements of the MobileNetV2 model.\n",
    "\n",
    "# img_to_array is a function provided by Keras that converts an image object into a NumPy array representation.\n",
    "\n",
    "# load_img is a function provided by Keras that loads an image file.              \n",
    "\n",
    "# to_categorical is a function in Keras that converts a class vector (integer labels) to binary class matrix representation. It is commonly used for one-hot encoding the target labels.      \n",
    "\n",
    "# LabelBinarizer is a class from scikit-learn that performs label binarization. It converts categorical labels into binary vectors suitable for training machine learning models.                       \n",
    "\n",
    "# train_test_split is a function from scikit-learn that splits the dataset into training and testing subsets. It is commonly used to create separate training and testing data for model evaluation. \n",
    "\n",
    "# classification_report is a function from scikit-learn that computes various classification metrics (such as precision, recall, F1-score) for evaluating the performance of a classification model.       \n",
    "\n",
    "# paths is a module from the imutils package. It provides functions to work with file paths, such as listing all file paths in a directory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c64ea55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the leraning rate, no of epochs and the batch size\n",
    "lrn_rt = 1e-4\n",
    "epoch =20\n",
    "batch = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd36ea8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "DIRECTORY = 'dataset/'\n",
    "CATEGORIES = ['with_mask','without_mask']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f99df534",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] loading images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\Downloads\\New folder (3)\\lib\\site-packages\\PIL\\Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "print('[info] loading images...')\n",
    "data =[]\n",
    "labels = []\n",
    "for category in CATEGORIES:\n",
    "    path = os.path.join(DIRECTORY,category)\n",
    "    for img in os.listdir(path):\n",
    "        img_path = os.path.join(path,img)\n",
    "        image = load_img(img_path ,target_size = (224,224))\n",
    "        image = img_to_array(image)\n",
    "        image = preprocess_input(image)\n",
    "        \n",
    "        data.append(image)\n",
    "        labels.append(category)\n",
    "        \n",
    "# The provided code snippet processes a directory structure containing images of different categories. It starts by initializing two empty lists: data and labels, which will be used to store the preprocessed image data and their corresponding labels, respectively.\n",
    "\n",
    "# Next, it iterates over each category in the categories list. For each category, it forms the complete path to the corresponding directory by concatenating the directory path and the current category name.\n",
    "\n",
    "# Within the category loop, it further iterates over each image file in the directory using os.listdir(). For each image file, it constructs the complete image path by joining the category's directory path and the image filename using os.path.join().\n",
    "\n",
    "# The code then loads the image using the load_img() function from Keras. It resizes the image to the target size of (224, 224) pixels.\n",
    "\n",
    "# After loading the image, it converts it to a NumPy array representation using img_to_array(). This conversion allows for further processing and manipulation of the image data.\n",
    "\n",
    "# The image is then preprocessed using preprocess_input(), a function specific to the MobileNetV2 model. This step involves scaling and normalizing the pixel values of the image array based on the requirements of the MobileNetV2 model.\n",
    "\n",
    "# Finally, the preprocessed image array is appended to the data list, while the corresponding category/label is appended to the labels list. This process is repeated for each image in each category's directory, resulting in a list of preprocessed image data and their respective labels.\n",
    "\n",
    "# Overall, this code snippet effectively loads, preprocesses, and collects image data along with their corresponding labels from a directory structure, making it ready for subsequent use in training a machine learning model.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "58218cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform encoding on the labels\n",
    "lb = LabelBinarizer()\n",
    "labels = lb.fit_transform(labels)\n",
    "labels = to_categorical(labels)\n",
    "\n",
    "\n",
    "data = np.array(data, dtype = 'float32')\n",
    "labels = np.array(labels)\n",
    "\n",
    "(trainx ,testx , trainy ,testy) = train_test_split(data,labels,test_size = 0.2,stratify = labels ,random_state = 42)\n",
    "\n",
    "# lb is an instance of the LabelBinarizer class from scikit-learn. It is used to convert categorical labels into binary vectors.\n",
    "\n",
    "# lb.fit_transform(labels) applies the fit_transform method to the labels list. It fits the label binarizer to the labels data and transforms the labels into binary vectors.\n",
    "\n",
    "# to_categorical is a function from Keras that converts the integer-encoded labels to one-hot encoded categorical labels.\n",
    "\n",
    "# The labels are transformed to a binary matrix representation, where each row corresponds to a sample and each column represents a category. The column corresponding to the true label is set to 1, while all other columns are set to 0\n",
    "\n",
    "# The data list, which contains the preprocessed image data, is converted to a NumPy array of type float32. This conversion ensures the data type is compatible with deep learning models.\n",
    "# Similarly, the labels list is converted to a NumPy array.\n",
    "\n",
    "# The train_test_split function from scikit-learn is used to split the data into training and testing datasets.\n",
    "\n",
    "# data and labels are the input arrays to be split.\n",
    "\n",
    "# test_size=0.2 specifies that 20% of the data should be allocated for testing, while the remaining 80% is used for training.\n",
    "\n",
    "# stratify=labels ensures that the class distribution is maintained in both the training and testing sets, which is important for balanced representation of classes.\n",
    "\n",
    "# random_state=42 sets a random seed for reproducibility, ensuring the same data split is obtained each time the code is run.\n",
    "\n",
    "# Overall, this code segment performs label binarization, one-hot encoding, data type conversion, and data splitting, preparing the data for training and testing a machine learning model.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1b80b122",
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct the training image generator for data augmentation\n",
    "aug = ImageDataGenerator(\n",
    "    rotation_range = 20,\n",
    "    zoom_range = 0.15,\n",
    "    width_shift_range = 0.2,\n",
    "    height_shift_range = 0.2,\n",
    "    shear_range = 0.15,\n",
    "    horizontal_flip = True,\n",
    "    fill_mode = 'nearest')\n",
    "\n",
    "# The provided code segment initializes an ImageDataGenerator object from the Keras library for data augmentation. This object applies various transformations to the images to generate augmented versions of the original data. Here's an explanation of each parameter:\n",
    "\n",
    "# rotation_range: It specifies the range, in degrees, within which the images can be randomly rotated. In this case, the range is set to 20, meaning the images can be rotated by a random angle between -20 and 20 degrees.\n",
    "\n",
    "# zoom_range: This parameter controls the range for random zooming of the images. A value of 0.15 indicates that the images can be zoomed in or out by a factor of up to 15% randomly.\n",
    "\n",
    "# width_shift_range and height_shift_range: These parameters control the range for randomly shifting the width and height of the images. A value of 0.2 means that the images can be horizontally or vertically shifted by up to 20% of the image width or height, respectively.\n",
    "\n",
    "# shear_range: It determines the range for applying random shearing transformations to the images. A value of 0.15 indicates that the images can be sheared by a random angle up to 15 degrees.\n",
    "\n",
    "# horizontal_flip: This parameter enables or disables horizontal flipping of the images. When set to True, there is a chance that the images will be horizontally flipped randomly.\n",
    "\n",
    "# fill_mode: It determines how to fill in the newly created pixels during the augmentation process. The 'nearest' mode fills the missing pixels with the nearest existing pixel values.\n",
    "\n",
    "# By configuring these augmentation parameters, the ImageDataGenerator object can generate augmented versions of the original images by applying random rotations, zooming, shifting, shearing, flipping, and filling in missing pixels. This data augmentation technique helps to increase the diversity and variability of the training data, which can improve the model's generalization and performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "56ebc1c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n"
     ]
    }
   ],
   "source": [
    "# load the mobilenetv2 network and ensure that the head fc layers are left off\n",
    "baseModel = MobileNetV2(weights ='imagenet',include_top = False, input_tensor = Input(shape=(224,224,3)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7f6e5335",
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct the head of the model which will be placed on top of the base model\n",
    "head = baseModel.output\n",
    "head = AveragePooling2D(pool_size =(7,7))(head)\n",
    "head = Flatten(name = 'flatten')(head)\n",
    "head = Dense(128,activation = 'relu')(head)\n",
    "head = Dropout(0.5)(head)\n",
    "head = Dense(2,activation = 'softmax')(head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8ec0e581",
   "metadata": {},
   "outputs": [],
   "source": [
    "# place the head fc model on top of the base model(this will be the model on which we will train)\n",
    "model = Model(baseModel.input,outputs = head)\n",
    "for layers in baseModel.layers:\n",
    "    layers.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e9590235",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] compiling model ...\n",
      "[info]  training head\n",
      "Epoch 1/20\n",
      "95/95 [==============================] - 114s 1s/step - loss: 0.4163 - accuracy: 0.8355 - val_loss: 0.1409 - val_accuracy: 0.9870\n",
      "Epoch 2/20\n",
      "95/95 [==============================] - 103s 1s/step - loss: 0.1424 - accuracy: 0.9647 - val_loss: 0.0731 - val_accuracy: 0.9896\n",
      "Epoch 3/20\n",
      "95/95 [==============================] - 103s 1s/step - loss: 0.1034 - accuracy: 0.9746 - val_loss: 0.0582 - val_accuracy: 0.9857\n",
      "Epoch 4/20\n",
      "95/95 [==============================] - 99s 1s/step - loss: 0.0792 - accuracy: 0.9802 - val_loss: 0.0473 - val_accuracy: 0.9896\n",
      "Epoch 5/20\n",
      "95/95 [==============================] - 98s 1s/step - loss: 0.0638 - accuracy: 0.9832 - val_loss: 0.0397 - val_accuracy: 0.9922\n",
      "Epoch 6/20\n",
      "95/95 [==============================] - 97s 1s/step - loss: 0.0596 - accuracy: 0.9838 - val_loss: 0.0380 - val_accuracy: 0.9935\n",
      "Epoch 7/20\n",
      "95/95 [==============================] - 97s 1s/step - loss: 0.0511 - accuracy: 0.9852 - val_loss: 0.0353 - val_accuracy: 0.9909\n",
      "Epoch 8/20\n",
      "95/95 [==============================] - 98s 1s/step - loss: 0.0456 - accuracy: 0.9855 - val_loss: 0.0327 - val_accuracy: 0.9935\n",
      "Epoch 9/20\n",
      "95/95 [==============================] - 95s 1s/step - loss: 0.0448 - accuracy: 0.9891 - val_loss: 0.0349 - val_accuracy: 0.9909\n",
      "Epoch 10/20\n",
      "95/95 [==============================] - 97s 1s/step - loss: 0.0340 - accuracy: 0.9911 - val_loss: 0.0311 - val_accuracy: 0.9935\n",
      "Epoch 11/20\n",
      "95/95 [==============================] - 98s 1s/step - loss: 0.0396 - accuracy: 0.9908 - val_loss: 0.0292 - val_accuracy: 0.9935\n",
      "Epoch 12/20\n",
      "95/95 [==============================] - 97s 1s/step - loss: 0.0391 - accuracy: 0.9868 - val_loss: 0.0288 - val_accuracy: 0.9935\n",
      "Epoch 13/20\n",
      "95/95 [==============================] - 97s 1s/step - loss: 0.0339 - accuracy: 0.9898 - val_loss: 0.0290 - val_accuracy: 0.9922\n",
      "Epoch 14/20\n",
      "95/95 [==============================] - 100s 1s/step - loss: 0.0346 - accuracy: 0.9891 - val_loss: 0.0339 - val_accuracy: 0.9909\n",
      "Epoch 15/20\n",
      "95/95 [==============================] - 97s 1s/step - loss: 0.0353 - accuracy: 0.9901 - val_loss: 0.0295 - val_accuracy: 0.9922\n",
      "Epoch 16/20\n",
      "95/95 [==============================] - 97s 1s/step - loss: 0.0326 - accuracy: 0.9898 - val_loss: 0.0258 - val_accuracy: 0.9935\n",
      "Epoch 17/20\n",
      "95/95 [==============================] - 97s 1s/step - loss: 0.0306 - accuracy: 0.9895 - val_loss: 0.0255 - val_accuracy: 0.9922\n",
      "Epoch 18/20\n",
      "95/95 [==============================] - 98s 1s/step - loss: 0.0238 - accuracy: 0.9931 - val_loss: 0.0256 - val_accuracy: 0.9922\n",
      "Epoch 19/20\n",
      "95/95 [==============================] - 18930s 201s/step - loss: 0.0292 - accuracy: 0.9908 - val_loss: 0.0251 - val_accuracy: 0.9935\n",
      "Epoch 20/20\n",
      "95/95 [==============================] - 109s 1s/step - loss: 0.0231 - accuracy: 0.9951 - val_loss: 0.0245 - val_accuracy: 0.9935\n"
     ]
    }
   ],
   "source": [
    "# # compile the model\n",
    "# from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# print('[info] compiling model ...')\n",
    "# opt = Adam(learning_rate = lrn_rt, decay = lrn_rt/epoch )\n",
    "# model.compile(loss ='binary_crossentropy', optimizder = opt , metrics = ['accuracy'])\n",
    "\n",
    "\n",
    "# train the head of the network\n",
    "\n",
    "# lrn_rt = 1e-4\n",
    "# epoch =20\n",
    "# batch = 32\n",
    "# decay_rate = lrn_rt / epoch\n",
    "from tensorflow.keras.optimizers import Adam as LegacyAdam\n",
    "\n",
    "# ...\n",
    "\n",
    "print('[info] compiling model ...')\n",
    "opt = LegacyAdam(learning_rate=lrn_rt , )\n",
    "model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "print('[info]  training head')\n",
    "H = model.fit(aug.flow(trainx,trainy,batch_size = batch),steps_per_epoch = len(trainx)//batch,validation_data = (testx,testy),\n",
    "             validation_steps = len(testx)//batch,epochs = epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5acdf78e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] evaluating network..\n",
      "24/24 [==============================] - 25s 922ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   with_mask       0.99      0.99      0.99       383\n",
      "without_mask       0.99      0.99      0.99       384\n",
      "\n",
      "    accuracy                           0.99       767\n",
      "   macro avg       0.99      0.99      0.99       767\n",
      "weighted avg       0.99      0.99      0.99       767\n",
      "\n",
      "[info]  saving mask detector model...\n"
     ]
    }
   ],
   "source": [
    "print('[info] evaluating network..')\n",
    "pred = model.predict(testx,batch_size = batch)\n",
    "# for each image in the testing set we need to find the index of the label with coressponding largest predicted probability\n",
    "pred = np.argmax(pred , axis =1)\n",
    "\n",
    "# show a nicely formatted vlassification report\n",
    "print(classification_report(testy.argmax(axis =1),pred , target_names = lb.classes_))\n",
    "# serialize the model to disk\n",
    "print('[info]  saving mask detector model...')\n",
    "model.save('mask_detector.model',save_format = 'h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b3277d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83cc3a72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ede8b45",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ff71fd8",
   "metadata": {},
   "source": [
    "## importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2b2c8948",
   "metadata": {},
   "outputs": [],
   "source": [
    " import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a92131df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7997f86",
   "metadata": {},
   "source": [
    "## data preprocessing \n",
    "### peprocessing the training set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "80286a83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6362 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "traindatagen = ImageDataGenerator(rescale = 1./225,shear_range= 0.2, zoom_range = 0.2, horizontal_flip =True)\n",
    "trainingset = traindatagen.flow_from_directory('datasets/facemask_detection/training_set', target_size = (64,64),batch_size = 32,class_mode = 'binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919d98c3",
   "metadata": {},
   "source": [
    "### preprocessing the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3b94917e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1191 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "testdatagen = ImageDataGenerator(rescale = 1./225,shear_range= 0.2, zoom_range = 0.2, horizontal_flip =True)\n",
    "testset = testdatagen.flow_from_directory('datasets/facemask_detection/test_set', target_size = (64,64),batch_size = 32,class_mode = 'binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02479cdc",
   "metadata": {},
   "source": [
    "## building CNN\n",
    "### initialising the CNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f6011991",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn =tf.keras.models.Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02df5ee",
   "metadata": {},
   "source": [
    "### convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "77cf917a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Conv2D(filters =32,kernel_size = 3,activation ='relu', input_shape =[64,64,3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208a1791",
   "metadata": {},
   "source": [
    "### poolinng "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "34f8b2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size = 2 ,strides = 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2a0aed",
   "metadata": {},
   "source": [
    "### adding the second layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "56b65408",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Conv2D(filters =32,kernel_size = 3,activation ='relu', input_shape =[64,64,3]))\n",
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size = 2 ,strides = 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85026ca0",
   "metadata": {},
   "source": [
    "### flattening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "266decf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ca7429",
   "metadata": {},
   "source": [
    "### full connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3e2e5cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Dense(units=128,activation = 'relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc882de6",
   "metadata": {},
   "source": [
    "### output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "00d3b045",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Dense(units=1,activation = 'sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d05871",
   "metadata": {},
   "source": [
    "### training the CNN on the training set and evaluating on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "efb96b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.compile(optimizer = 'adam',loss = 'binary_crossentropy',metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275747ba",
   "metadata": {},
   "source": [
    "## training the CNN\n",
    "### compiling the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ac290c3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "199/199 [==============================] - 75s 367ms/step - loss: 0.3825 - accuracy: 0.8225 - val_loss: 0.2065 - val_accuracy: 0.9362\n",
      "Epoch 2/25\n",
      "199/199 [==============================] - 69s 344ms/step - loss: 0.2647 - accuracy: 0.8922 - val_loss: 0.1887 - val_accuracy: 0.9353\n",
      "Epoch 3/25\n",
      "199/199 [==============================] - 68s 343ms/step - loss: 0.2212 - accuracy: 0.9088 - val_loss: 0.1595 - val_accuracy: 0.9370\n",
      "Epoch 4/25\n",
      "199/199 [==============================] - 71s 359ms/step - loss: 0.2003 - accuracy: 0.9164 - val_loss: 0.1738 - val_accuracy: 0.9404\n",
      "Epoch 5/25\n",
      "199/199 [==============================] - 69s 348ms/step - loss: 0.1835 - accuracy: 0.9272 - val_loss: 0.1721 - val_accuracy: 0.9387\n",
      "Epoch 6/25\n",
      "199/199 [==============================] - 161s 810ms/step - loss: 0.1606 - accuracy: 0.9334 - val_loss: 0.1337 - val_accuracy: 0.9563\n",
      "Epoch 7/25\n",
      "199/199 [==============================] - 167s 841ms/step - loss: 0.1410 - accuracy: 0.9423 - val_loss: 0.1029 - val_accuracy: 0.9597\n",
      "Epoch 8/25\n",
      "199/199 [==============================] - 158s 791ms/step - loss: 0.1340 - accuracy: 0.9478 - val_loss: 0.2013 - val_accuracy: 0.9437\n",
      "Epoch 9/25\n",
      "199/199 [==============================] - 177s 887ms/step - loss: 0.1168 - accuracy: 0.9549 - val_loss: 0.1010 - val_accuracy: 0.9715\n",
      "Epoch 10/25\n",
      "199/199 [==============================] - 148s 742ms/step - loss: 0.1114 - accuracy: 0.9560 - val_loss: 0.0855 - val_accuracy: 0.9723\n",
      "Epoch 11/25\n",
      "199/199 [==============================] - 54s 271ms/step - loss: 0.0995 - accuracy: 0.9651 - val_loss: 0.0974 - val_accuracy: 0.9689\n",
      "Epoch 12/25\n",
      "199/199 [==============================] - 54s 269ms/step - loss: 0.0934 - accuracy: 0.9648 - val_loss: 0.2827 - val_accuracy: 0.9379\n",
      "Epoch 13/25\n",
      "199/199 [==============================] - 56s 284ms/step - loss: 0.0851 - accuracy: 0.9668 - val_loss: 0.1156 - val_accuracy: 0.9715\n",
      "Epoch 14/25\n",
      "199/199 [==============================] - 53s 266ms/step - loss: 0.0816 - accuracy: 0.9714 - val_loss: 0.0839 - val_accuracy: 0.9748\n",
      "Epoch 15/25\n",
      "199/199 [==============================] - 61s 308ms/step - loss: 0.0663 - accuracy: 0.9766 - val_loss: 0.0852 - val_accuracy: 0.9748\n",
      "Epoch 16/25\n",
      "199/199 [==============================] - 54s 271ms/step - loss: 0.0748 - accuracy: 0.9719 - val_loss: 0.1343 - val_accuracy: 0.9647\n",
      "Epoch 17/25\n",
      "199/199 [==============================] - 53s 267ms/step - loss: 0.0592 - accuracy: 0.9802 - val_loss: 0.0928 - val_accuracy: 0.9723\n",
      "Epoch 18/25\n",
      "199/199 [==============================] - 53s 265ms/step - loss: 0.0571 - accuracy: 0.9802 - val_loss: 0.1192 - val_accuracy: 0.9673\n",
      "Epoch 19/25\n",
      "199/199 [==============================] - 51s 256ms/step - loss: 0.0519 - accuracy: 0.9799 - val_loss: 0.1070 - val_accuracy: 0.9723\n",
      "Epoch 20/25\n",
      "199/199 [==============================] - 52s 261ms/step - loss: 0.0438 - accuracy: 0.9840 - val_loss: 0.0932 - val_accuracy: 0.9715\n",
      "Epoch 21/25\n",
      "199/199 [==============================] - 105s 529ms/step - loss: 0.0510 - accuracy: 0.9818 - val_loss: 0.1145 - val_accuracy: 0.9706\n",
      "Epoch 22/25\n",
      "199/199 [==============================] - 102s 511ms/step - loss: 0.0416 - accuracy: 0.9854 - val_loss: 0.0877 - val_accuracy: 0.9740\n",
      "Epoch 23/25\n",
      "199/199 [==============================] - 56s 279ms/step - loss: 0.0352 - accuracy: 0.9877 - val_loss: 0.1168 - val_accuracy: 0.9723\n",
      "Epoch 24/25\n",
      "199/199 [==============================] - 51s 254ms/step - loss: 0.0396 - accuracy: 0.9863 - val_loss: 0.1156 - val_accuracy: 0.9723\n",
      "Epoch 25/25\n",
      "199/199 [==============================] - 53s 269ms/step - loss: 0.0369 - accuracy: 0.9876 - val_loss: 0.1625 - val_accuracy: 0.9589\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c86d4d0b20>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn.fit(x= trainingset, validation_data = testset, epochs = 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba46cc26",
   "metadata": {},
   "source": [
    "### making predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "bea8ae89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 54ms/step\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import keras.utils as image\n",
    "test_image = image.load_img('datasets/facemask_detection/single_prediction/check_2.jpg', target_size = (64, 64))\n",
    "test_image = image.img_to_array(test_image)\n",
    "test_image = np.expand_dims(test_image, axis = 0)\n",
    "result = cnn.predict(test_image)\n",
    "trainingset.class_indices \n",
    "if result[0][0] == 1:\n",
    "  prediction = 'without mask'\n",
    "else:\n",
    "  prediction = ' mask'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5a4037cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " mask\n"
     ]
    }
   ],
   "source": [
    "print(prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9ced8056",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.]]\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a29ba868",
   "metadata": {},
   "source": [
    "## the end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
